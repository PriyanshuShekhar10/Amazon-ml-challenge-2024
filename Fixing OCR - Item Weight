{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28","mount_file_id":"18YCVhq3o2AM0cI2l80oodaNYqOwgpwj7","authorship_tag":"ABX9TyOifsZbqeiqn7hfHPvYM9P0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["**Deskewing with OpenCV**: This part uses OpenCV to compute the skew angle of the text and rotates the image to correct it. This is particularly useful for images where the text is tilted or rotated.\n","\n","**Adaptive Thresholding**: This improves the image contrast, especially for images where the text has different fonts or low contrast against the background.\n","\n","**Correct Orientation:** The ImageOps.exif_transpose function is used to handle the image orientation based on EXIF data, which corrects issues where the image is rotated due to metadata.\n","\n","**OCR Configuration** (--psm 6): This configuration assumes that the image contains a single block of text, which can improve OCR accuracy for images with straightforward text layouts"],"metadata":{"id":"F_dTeqDkPEYc"}},{"cell_type":"code","source":["!pip install easyocr\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"],"metadata":{"id":"jhmQlIOLei1r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726233969736,"user_tz":-330,"elapsed":8711,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"f44acd06-5567-4f12-ed95-f45831a7ecaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting easyocr\n","  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.34.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.30)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n","Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n","Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install requests pytesseract spacy pillow"],"metadata":{"id":"PPpVuyrsP5hk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get install tesseract-ocr"],"metadata":{"id":"30j0uTIXP3Cf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# OCR USING TESSERACT"],"metadata":{"id":"WiskJ3CW_8tk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtVfV5npOOMa","executionInfo":{"status":"ok","timestamp":1726212201843,"user_tz":-330,"elapsed":254597,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"61814372-58c5-4955-a9df-9d4cca5663e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing image: https://m.media-amazon.com/images/I/61I9XdN6OFL.jpg\n","No unit found for number '500.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61BZ4zrjZXL.jpg\n","Found unit 'g' for number '0.709'\n","Processing image: https://m.media-amazon.com/images/I/612mrlqiI4L.jpg\n","No unit found for number '0.709' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/617Tl40LOXL.jpg\n","No unit found for number '1400' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61QsBSE7jgL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/81xsq6vf2qL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/71DiLRHeZdL.jpg\n","No unit found for number '1400' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/91Cma3RzseL.jpg\n","No unit found for number '1400' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71jBLhmTNlL.jpg\n","No unit found for number '1400' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81N73b5khVL.jpg\n","No unit found for number '30.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61oMj2iXOuL.jpg\n","Found unit 'kg' for number '10'\n","Processing image: https://m.media-amazon.com/images/I/91LPf6OjV9L.jpg\n","No unit found for number '3.53' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81fOxWWWKYL.jpg\n","No unit found for number '3.53' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81dzao1Ob4L.jpg\n","Found unit 'oz' for number '53'\n","Processing image: https://m.media-amazon.com/images/I/91-iahVGEDL.jpg\n","No unit found for number '100' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81S2+GnYpTL.jpg\n","No unit found for number '200' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81e2YtCOKvL.jpg\n","No unit found for number '1' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81RNsNEM1EL.jpg\n","No unit found for number '200' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/91prZeizZnL.jpg\n","No unit found for number '200' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/31EvJszFVfL.jpg\n","No unit found for number '200' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61sQ+qAKr4L.jpg\n","Found unit 'g' for number '2.7'\n","Processing image: https://m.media-amazon.com/images/I/81x77l2T5NL.jpg\n","No unit found for number '112' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71nywfWZUwL.jpg\n","No unit found for number '4.1' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/51WsuKKAVrL.jpg\n","No unit found for number '158.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61XGDKap+JL.jpg\n","No unit found for number '158.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/715vVcWJxGL.jpg\n","No unit found for number '5000' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/613v+2W4UwL.jpg\n","Found unit 'g' for number '18.55'\n","Processing image: https://m.media-amazon.com/images/I/71+fn9TWQmL.jpg\n","Found unit 'g' for number '18.55'\n","Processing image: https://m.media-amazon.com/images/I/71aKgRRQ2wL.jpg\n","Found unit '8' for number '18.55'\n","Processing image: https://m.media-amazon.com/images/I/71rKXZJrh4L.jpg\n","Found unit 'g' for number '18.55'\n","Processing image: https://m.media-amazon.com/images/I/71D824lbRvL.jpg\n","Found unit 'g' for number '18.55'\n","Processing image: https://m.media-amazon.com/images/I/71004c9tzfL.jpg\n","No unit found for number '50.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/51bQPPtMqYL.jpg\n","No unit found for number '26.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71IUuTJ8QwL.jpg\n","No unit found for number '330.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/915JHkwtcrL.jpg\n","No unit found for number '31.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71cjrYndwIL.jpg\n","Found unit '0z' for number '0.35'\n","Processing image: https://m.media-amazon.com/images/I/81hnk2WXO3L.jpg\n","No unit found for number '35.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/41wvffSxB4L.jpg\n","Found unit 'g' for number '15.5'\n","Processing image: https://m.media-amazon.com/images/I/91cErO-KbLL.jpg\n","No unit found for number '200.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61WFh8RCQYL.jpg\n","Found unit 'kg' for number '0.8'\n","Processing image: https://m.media-amazon.com/images/I/711SATIDrmL.jpg\n","No unit found for number '169.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61x6RSjwQIL.jpg\n","No unit found for number '10.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/613BeFNwHcL.jpg\n","No unit found for number '7.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61hWZdkq6WL.jpg\n","No unit found for number '750.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71E7CU55dcL.jpg\n","No unit found for number '160.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61c+hSNnnZL.jpg\n","No unit found for number '270.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/915w0BdW-gL.jpg\n","No unit found for number '500' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61sx0ezNNLL.jpg\n","No unit found for number '1' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71ldprwbKrL.jpg\n","No unit found for number '10' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71E9iF-bmKL.jpg\n","No unit found for number '1' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71sWRp1SNwL.jpg\n","No unit found for number '2.2' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61SmT8pkLtL.jpg\n","Found unit 'grams' for number '2'\n","Processing image: https://m.media-amazon.com/images/I/71ZtDgGX+iL.jpg\n","Found unit 'g' for number '8.1'\n","Processing image: https://m.media-amazon.com/images/I/413FQB0ZMLL.jpg\n","No unit found for number '2' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/41EjbFu-+yL.jpg\n","No unit found for number '10' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71dWDwMhWmS.jpg\n","No unit found for number '500.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61d6Kj80QSL.jpg\n","No unit found for number '200.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71bvOuz9w1L.jpg\n","No unit found for number '200.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71l0M0tMGjL.jpg\n","No unit found for number '200.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71Lpqdrpi4L.jpg\n","No unit found for number '200.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71jLIbCcwOL.jpg\n","No unit found for number '100.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/718EdwGgyVL.jpg\n","No unit found for number '200.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/713twQgCHSL.jpg\n","No unit found for number '5.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61I0O1qJbhS.jpg\n","No unit found for number '60.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/716AQpAJjZL.jpg\n","No unit found for number '227' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71FVeRd2jqL.jpg\n","No unit found for number '100' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81njuNSPdjL.jpg\n","No unit found for number '190.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/51xfRlxWIXL.jpg\n","No unit found for number '100.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71duwM3SjpL.jpg\n","Found unit 'mg' for number '600'\n","Processing image: https://m.media-amazon.com/images/I/612xIhPMHqL.jpg\n","Found unit 'g' for number '3.2'\n","Processing image: https://m.media-amazon.com/images/I/51b9JEHOriL.jpg\n","Found unit 'g' for number '6.5'\n","Processing image: https://m.media-amazon.com/images/I/81lgxfKqUUL.jpg\n","No unit found for number '160.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/814sAvV89SL.jpg\n","No unit found for number '42.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61cMeogK8gL.jpg\n","No unit found for number '26.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71WLYfmMqQL.jpg\n","No unit found for number '50.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61Dq3LRei9L.jpg\n","No unit found for number '10.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61kyBEJYDeL.jpg\n","No unit found for number '500' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71uQmsTESvL.jpg\n","No unit found for number '500' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71jG8BOi4WL.jpg\n","No unit found for number '16.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61390hosjFL.jpg\n","No unit found for number '8' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71YiRw3e1lL.jpg\n","No unit found for number '3.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/719poaEkdEL.jpg\n","No unit found for number '750.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/41njJJg-ANL.jpg\n","No unit found for number '8.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71qEriwsbIL.jpg\n","No unit found for number '23.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/514M5CrBK4L.jpg\n","Found unit 'g' for number '9.8'\n","Processing image: https://m.media-amazon.com/images/I/61qOr-yqaVL.jpg\n","No unit found for number '198.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61xxqfM2EwL.jpg\n","No unit found for number '9.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81yG9eUKvxL.jpg\n","No unit found for number '2.5' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61FMOl299lL.jpg\n","No unit found for number '50.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/51T1Kh7WWGL.jpg\n","Found unit 'g' for number '4.1'\n","Processing image: https://m.media-amazon.com/images/I/71G8XyPqcIL.jpg\n","Found unit 'g' for number '9.2'\n","Processing image: https://m.media-amazon.com/images/I/61Hhju+qn9L.jpg\n","No unit found for number '600.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/51Alw-4+zJL.jpg\n","No unit found for number '20.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81l6zfwKG7L.jpg\n","No unit found for number '13.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71eATPu53YL.jpg\n","No unit found for number '13.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81R8CrWXthL.jpg\n","No unit found for number '1000.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71qiWPgPcUS.jpg\n","No unit found for number '264.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/81nyHjPXg-L.jpg\n","No unit found for number '34' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/71EREAIkJZS.jpg\n","Found unit 'grams' for number '72.57'\n","Processing image: https://m.media-amazon.com/images/I/61bpzE0p4yL.jpg\n","Found unit 'g' for number '100'\n","\n","Final extracted values and units:\n","[('0.709', 'g'), ('1400', 'mg'), ('1400', 'mg'), ('10', 'kg'), ('53', 'oz'), ('2.7', 'g'), ('18.55', 'g'), ('18.55', 'g'), ('18.55', '8'), ('18.55', 'g'), ('18.55', 'g'), ('0.35', '0z'), ('15.5', 'g'), ('0.8', 'kg'), ('2', 'grams'), ('8.1', 'g'), ('600', 'mg'), ('3.2', 'g'), ('6.5', 'g'), ('9.8', 'g'), ('4.1', 'g'), ('9.2', 'g'), ('72.57', 'grams'), ('100', 'g')]\n","Unique units: {'oz', 'g', 'kg', '8', 'grams', '0z', 'mg'}\n"]}],"source":["import requests\n","from PIL import Image, ImageFilter\n","from io import BytesIO\n","import pandas as pd\n","import re\n","import pytesseract\n","\n","# Function to download and open an image\n","def download_image(url):\n","    try:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises an error for HTTP errors\n","        return Image.open(BytesIO(response.content))\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Failed to download image from {url}. Error: {e}\")\n","        return None\n","\n","# Load the training data\n","train_data = pd.read_csv('/content/drive/MyDrive/66e31d6ee96cd_student_resource_3/student_resource 3/dataset/train.csv')\n","\n","# Filter the DataFrame to include only rows where entity_name is \"item_weight\"\n","filtered_data = train_data[train_data['entity_name'] == 'item_weight']\n","filtered_data = filtered_data.head(100)\n","\n","# Function to extract number and suffix from entity value\n","def extract_number_and_suffix(entity_value):\n","    match = re.match(r'(\\d+\\.?\\d*)\\s*(\\w+)', entity_value)\n","    if match:\n","        number, suffix = match.groups()\n","        return number, suffix.lower()\n","    return None, None\n","\n","# Array to store results\n","results_array = []\n","\n","# Iterate through each row in the filtered dataframe to extract weights from OCR results\n","for index, row in filtered_data.iterrows():\n","    image_url = row['image_link']  # Assuming the image URL is in a column named 'image_link'\n","\n","    print(f\"Processing image: {image_url}\")\n","\n","    # Download and perform OCR on the image\n","    image = download_image(image_url)\n","\n","    if image is not None:\n","        # Sharpen the image\n","        sharpened_image = image.filter(ImageFilter.SHARPEN)\n","\n","        # Perform OCR\n","        ocr_result = pytesseract.image_to_string(sharpened_image)\n","\n","        # Extract number and suffix from entity_value column\n","        entity_value = str(row['entity_value'])\n","        number, suffix = extract_number_and_suffix(entity_value)\n","\n","        if number:\n","            # Find the unit that follows the numerical value in the OCR result\n","            unit_pattern = rf'{number}\\s*(\\w+)'\n","            unit_match = re.search(unit_pattern, ocr_result, re.IGNORECASE)\n","            if unit_match:\n","                unit = unit_match.group(1).lower()\n","                print(f\"Found unit '{unit}' for number '{number}'\")\n","                results_array.append((number, unit))\n","            else:\n","                print(f\"No unit found for number '{number}' in OCR result.\")\n","        else:\n","            print(f\"No valid number found in entity_value: {entity_value}\")\n","    else:\n","        print(\"Skipping image due to download error.\\n\")\n","\n","# Output the array of results\n","print(\"\\nFinal extracted values and units:\")\n","print(results_array)\n","\n","# Extract unique units from results_array\n","unique_units = set([unit for _, unit in results_array])\n","\n","print(\"Unique units:\", unique_units)\n"]},{"cell_type":"markdown","source":["# EasyOCR (better than tesseract)"],"metadata":{"id":"mSPEpUDcSNpn"}},{"cell_type":"markdown","source":[],"metadata":{"id":"RZqDTMG0QvgX"}},{"cell_type":"code","source":["import requests\n","from PIL import Image, ImageFilter\n","from io import BytesIO\n","import pandas as pd\n","import re\n","import easyocr\n","import cv2 # import the cv2 module\n","import numpy as np # import numpy\n","\n","# Function to download and open an image\n","def download_image(url):\n","    try:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises an error for HTTP errors\n","        return Image.open(BytesIO(response.content))\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Failed to download image from {url}. Error: {e}\")\n","        return None\n","\n","# Load the training data\n","train_data = pd.read_csv('/content/drive/MyDrive/66e31d6ee96cd_student_resource_3/student_resource 3/dataset/train.csv')\n","\n","# Filter the DataFrame to include only rows where entity_name is \"item_weight\"\n","filtered_data = train_data[train_data['entity_name'] == 'item_weight']\n","filtered_data = filtered_data.head(10)\n","\n","# Function to extract number and suffix from entity value\n","def extract_number_and_suffix(entity_value):\n","    match = re.match(r'(\\d+\\.?\\d*)\\s*(\\w+)', entity_value)\n","    if match:\n","        number, suffix = match.groups()\n","        return number, suffix.lower()\n","    return None, None\n","\n","# Initialize EasyOCR Reader\n","reader = easyocr.Reader(['en'], gpu=True)  # Specify language(s) as needed\n","\n","# Array to store results\n","results_array = []\n","\n","# Iterate through each row in the filtered dataframe to extract weights from OCR results\n","for index, row in filtered_data.iterrows():\n","    image_url = row['image_link']  # Assuming the image URL is in a column named 'image_link'\n","\n","    print(f\"Processing image: {image_url}\")\n","\n","    # Download and perform OCR on the image\n","    image = download_image(image_url)\n","\n","    if image is not None:\n","        # Sharpen the image\n","        sharpened_image = image.filter(ImageFilter.SHARPEN)\n","\n","        # Convert PIL image to OpenCV format for EasyOCR\n","        open_cv_image = cv2.cvtColor(np.array(sharpened_image), cv2.COLOR_RGB2BGR)\n","\n","        # Perform OCR using EasyOCR\n","        ocr_result = reader.readtext(open_cv_image, detail=0)\n","        ocr_text = ' '.join(ocr_result)  # Combine list of text lines into a single string\n","\n","        # Extract number and suffix from entity_value column\n","        entity_value = str(row['entity_value'])\n","        number, suffix = extract_number_and_suffix(entity_value)\n","\n","        if number:\n","            # Find the unit that follows the numerical value in the OCR result\n","            unit_pattern = rf'{number}\\s*(\\w+)'\n","            unit_match = re.search(unit_pattern, ocr_text, re.IGNORECASE)\n","            if unit_match:\n","                unit = unit_match.group(1).lower()\n","                print(f\"Found unit '{unit}' for number '{number}'\")\n","                results_array.append((number, unit))\n","            else:\n","                print(f\"No unit found for number '{number}' in OCR result.\")\n","        else:\n","            print(f\"No valid number found in entity_value: {entity_value}\")\n","    else:\n","        print(\"Skipping image due to download error.\\n\")\n","\n","# Output the array of results\n","print(\"\\nFinal extracted values and units:\")\n","print(results_array)\n","\n","# Extract unique units from results_array\n","unique_units = set([unit for _, unit in results_array])\n","\n","print(\"Unique units:\", unique_units)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tEjY6vzZelEL","executionInfo":{"status":"ok","timestamp":1726220660588,"user_tz":-330,"elapsed":12180,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"c9870adf-b84a-4f95-e050-3d7196fab63e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Processing image: https://m.media-amazon.com/images/I/61I9XdN6OFL.jpg\n","No unit found for number '500.0' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/61BZ4zrjZXL.jpg\n","Found unit 'g' for number '0.709'\n","Processing image: https://m.media-amazon.com/images/I/612mrlqiI4L.jpg\n","No unit found for number '0.709' in OCR result.\n","Processing image: https://m.media-amazon.com/images/I/617Tl40LOXL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/61QsBSE7jgL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/81xsq6vf2qL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/71DiLRHeZdL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/91Cma3RzseL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/71jBLhmTNlL.jpg\n","Found unit 'mg' for number '1400'\n","Processing image: https://m.media-amazon.com/images/I/81N73b5khVL.jpg\n","No unit found for number '30.0' in OCR result.\n","\n","Final extracted values and units:\n","[('0.709', 'g'), ('1400', 'mg'), ('1400', 'mg'), ('1400', 'mg'), ('1400', 'mg'), ('1400', 'mg'), ('1400', 'mg')]\n","Unique units: {'mg', 'g'}\n"]}]},{"cell_type":"code","source":[" units  = {'lb', 'lbs', 'pound', 'pounds', 'lb.', 'lbs.', 'pd', 'pds', 'pd.', 'pnd', 'pondo',\n","    'kg', 'kgs', 'kilogram', 'kilograms', 'kilo', 'kilos', 'kg.', 'kgs.', 'kilogramme', 'kilogrammes',\n","    'kilog', 'kgr', 'kgram', 'kgrm','g', 'gm', 'gms', 'gram', 'grams', 'gramme', 'grammes', 'g.', 'gm.', 'gms.', 'grm', 'grms', 'gr.', 'gmme'\n"," }\n","\n","# Extract unique units from results_array\n","rejected_units = set([unit for _, unit in results_array])\n","\n","print(\"Unique units:\", unique_units)\n","\n","# Human in the loop - Manual review and selection\n","selected_units = []\n","rejected_units = []\n","\n","for unit in unique_units:\n","    print(f\"\\nUnit: {unit}\")\n","    choice = input(\"Select (s) or Reject (r)? \")\n","    if choice.lower() == 's':\n","        selected_units.append(unit)\n","    elif choice.lower() == 'r':\n","        rejected_units.append(unit)\n","    else:\n","        print(\"Invalid choice. Skipping.\")\n","\n","print(\"\\nSelected units:\", selected_units)\n","print(\"Rejected units:\", rejected_units)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKTDwaRhpaL5","executionInfo":{"status":"ok","timestamp":1726220724707,"user_tz":-330,"elapsed":14056,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"204dbc0d-9558-4046-b5a0-b26125e16b9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique units: {'mg', 'g'}\n","\n","Unit: mg\n","Select (s) or Reject (r)? s\n","\n","Unit: g\n","Select (s) or Reject (r)? s\n","\n","Selected units: ['mg', 'g']\n","Rejected units: []\n"]}]},{"cell_type":"code","source":["import re\n","import easyocr\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","import cv2\n","import numpy as np\n","\n","# List of weight unit variations\n","weight_units = [\n","    # Pound Variations\n","    'lb', 'lbs', 'pound', 'pounds', 'lb.', 'lbs.', 'pd', 'pds', 'pd.', 'pnd', 'pondo',\n","\n","    # Kilogram Variations\n","    'kg', 'kgs', 'kilogram', 'kilograms', 'kilo', 'kilos', 'kg.', 'kgs.', 'kilogramme', 'kilogrammes',\n","    'kilog', 'kgr', 'kgram', 'kgrm',\n","\n","    # Gram Variations\n","    'g', 'gm', 'gms', 'gram', 'grams', 'gramme', 'grammes', 'g.', 'gm.', 'gms.', 'grm', 'grms', 'gr.', 'gmme',\n","\n","    # Ounce Variations\n","    'oz', 'ounce', 'ounces', 'oz.', 'ozs', 'ozs.'\n","]\n","\n","\n","# Initialize EasyOCR Reader\n","reader = easyocr.Reader(['en'], gpu=True)  # Using GPU if available\n","\n","# Function to download and open an image\n","def download_image(url):\n","    try:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # Raises an error for HTTP errors\n","        return Image.open(BytesIO(response.content))\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Failed to download image from {url}. Error: {e}\")\n","        return None\n","\n","# Function to extract the first number and its corresponding unit (if any) from OCR text\n","def extract_first_number_with_unit(ocr_result):\n","    # Combine OCR results into a single string\n","    ocr_text = ' '.join(ocr_result)\n","\n","    # Normalize numbers by removing spaces within numbers (e.g., '1,500' should remain '1,500')\n","    ocr_text = re.sub(r'(\\d),(\\d)', r'\\1\\2', ocr_text)  # Remove commas in numbers\n","\n","    # Use regex to find all occurrences of numbers (including those with commas) and following words\n","    matches = re.findall(r'(\\d+\\.?\\d*|\\d{1,3}(?:,\\d{3})*)\\s*(\\w+)?', ocr_text)\n","\n","    for number, word in matches:\n","        # Check if the following word is a recognized unit\n","        if word and word.lower() in weight_units:\n","            return number.replace(',', ''), word.lower()  # Remove commas for the numeric value\n","\n","    # If no unit matches, return the first detected number\n","    if matches:\n","        return matches[0][0].replace(',', ''), None  # Remove commas for the numeric value\n","\n","    # If no numbers are found, return None\n","    return None, None\n","\n","# URL of the image\n","image_url = 'https://m.media-amazon.com/images/I/21bfrFeArAL.jpg'\n","\n","# Download the image\n","image = download_image(image_url)\n","\n","if image:\n","    # Convert PIL image to OpenCV format for EasyOCR\n","    open_cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","\n","    # Perform OCR using EasyOCR\n","    ocr_result = reader.readtext(open_cv_image, detail=0)\n","\n","    # Extract the first number with its corresponding unit (if any)\n","    number, unit = extract_first_number_with_unit(ocr_result)\n","\n","    if unit:\n","        print(f\"First number with matching unit: {number} {unit}\")\n","    elif number:\n","        print(f\"First number without matching unit: {number}\")\n","    else:\n","        print(\"No number found.\")\n","else:\n","    print(\"Failed to process the image.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI4f4JLgznZi","executionInfo":{"status":"ok","timestamp":1726233992041,"user_tz":-330,"elapsed":20219,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"5382b870-8663-4288-cb14-b3b439f8fcf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n","WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"]},{"output_type":"stream","name":"stdout","text":["Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"]},{"output_type":"stream","name":"stdout","text":["Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(model_path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["First number without matching unit: 91\n"]}]},{"cell_type":"markdown","source":["# FIXING TEXT ROATATION"],"metadata":{"id":"2Zvm4VAR4Tdc"}},{"cell_type":"code","source":[],"metadata":{"id":"3eKxwG-gSY0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import easyocr\n","import numpy as np\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","from google.colab.patches import cv2_imshow # import cv2_imshow from google.colab.patches\n","\n","# Load the image\n","image_path = 'https://m.media-amazon.com/images/I/91LPf6OjV9L.jpg'\n","\n","# Download the image from the URL\n","try:\n","    response = requests.get(image_path, stream=True)\n","    response.raise_for_status()\n","    image = Image.open(BytesIO(response.content))\n","    # Convert PIL Image to OpenCV format\n","    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error downloading image: {e}\")\n","    image = None\n","\n","# Initialize EasyOCR reader\n","reader = easyocr.Reader(['en'])\n","\n","if image is not None:\n","    # Detect text with bounding boxes\n","    results = reader.readtext(image, detail=1)  # Set detail=1 to get bounding boxes\n","\n","    # Loop through detected text\n","    for (bbox, text, prob) in results:\n","        # Extract the bounding box coordinates\n","        (top_left, top_right, bottom_right, bottom_left) = bbox\n","\n","        # Compute the center of the bounding box\n","        center_x = int((top_left[0] + bottom_right[0]) / 2)\n","        center_y = int((top_left[1] + bottom_right[1]) / 2)\n","\n","        # Calculate the rotation angle of the bounding box\n","        angle = np.degrees(np.arctan2(bottom_right[1] - bottom_left[1], bottom_right[0] - bottom_left[0]))\n","\n","        # Rotate the image to correct the orientation\n","        (h, w) = image.shape[:2]\n","        M = cv2.getRotationMatrix2D((center_x, center_y), -angle, 1.0)\n","        rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n","\n","        # (Optional) Display the rotated image\n","        cv2_imshow(rotated) # Use cv2_imshow instead of cv2.imshow\n","        cv2.waitKey(0)\n","\n","    # Save or further process the corrected image\n","    cv2.imwrite('content/corrected_image.png', rotated)\n","else:\n","    print(\"Could not load the image.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_cJOKD51zKpGsakrkwTwBwZxIykZoAr2"},"id":"zpLpDlvV1NPH","outputId":"52d16a2a-cfda-4432-b061-3cf5cd90e7d7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import easyocr\n","\n","def detect_text_orientation(image_path):\n","    # Read the image using OpenCV\n","    # image = cv2.imread(image_path)\n","\n","    # Download the image from the URL\n","    try:\n","        response = requests.get(image_path, stream=True)\n","        response.raise_for_status()\n","        image = Image.open(BytesIO(response.content))\n","        # Convert PIL Image to OpenCV format\n","        image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error downloading image: {e}\")\n","        image = None\n","\n","    if image is not None:\n","        # Convert the image to grayscale\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","        # Use OpenCV's built-in text detector to find the orientation\n","        coords = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n","\n","        # Initialize variables for angle calculation\n","        angle_sum = 0\n","        angle_count = 0\n","\n","        # Calculate the angle for each contour detected\n","        for c in coords:\n","            rect = cv2.minAreaRect(c)\n","            angle = rect[-1]\n","            if angle < -45:\n","                angle = 90 + angle\n","            angle_sum += angle\n","            angle_count += 1\n","\n","        # Average the detected angles to determine the main text orientation\n","        average_angle = angle_sum / max(angle_count, 1)\n","\n","        # Rotate the image if the angle is significant\n","        if abs(average_angle) > 5:  # Adjust this threshold as needed\n","            (h, w) = gray.shape[:2]\n","            center = (w // 2, h // 2)\n","            M = cv2.getRotationMatrix2D(center, -average_angle, 1.0)\n","            rotated_image = cv2.warpAffine(image, M, (w, h))\n","        else:\n","            rotated_image = image\n","        return rotated_image\n","    else:\n","        return None\n","\n","def perform_ocr_with_easyocr(image_path):\n","    # Detect text orientation and correct it\n","    corrected_image = detect_text_orientation(image_path)\n","\n","    # Convert corrected image to grayscale (EasyOCR works well on grayscale images)\n","    gray_corrected_image = cv2.cvtColor(corrected_image, cv2.COLOR_BGR2GRAY)\n","\n","    # Initialize EasyOCR reader\n","    reader = easyocr.Reader(['en'])  # Add more languages as needed\n","\n","    # Perform OCR using EasyOCR\n","    result = reader.readtext(gray_corrected_image)\n","\n","    # Extract and combine the text results\n","    extracted_text = ' '.join([text[1] for text in result])\n","\n","    return extracted_text\n","\n","\n","    # Path to your image\n","image_path = 'https://m.media-amazon.com/images/I/91LPf6OjV9L.jpg'\n","\n","    # Perform OCR with orientation correction using EasyOCR\n","ocr_result = perform_ocr_with_easyocr(image_path)\n","print(\"Extracted Text:\\n\", ocr_result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSt2EJmZ4io0","executionInfo":{"status":"ok","timestamp":1726224667964,"user_tz":-330,"elapsed":4938,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"a9bc9b5a-fd65-4a3e-9706-f9ed10fae2af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device))\n"]},{"output_type":"stream","name":"stdout","text":["Extracted Text:\n"," 8I 5' V p ! 1 { 5' 79; 10 I '1 0 9 6 V 8 5 0 4 8 0 8 2 2 '2 1 PREMIUM , 3 1 1 8 8 LNJIJION ( 6 3 2 5 1 3 1 8 2 2 7 J 9 1 4 8 0 0 1 1 1 2 8 8 2 8 0 I 0 1\n"]}]},{"cell_type":"code","source":["!pip install pyobjc\n"],"metadata":{"id":"aWR5aXxnAZZV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726231584035,"user_tz":-330,"elapsed":1399,"user":{"displayName":"Nalin","userId":"12386870817090352164"}},"outputId":"202d1d0a-217d-49c4-cfe6-d503f986e216"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyobjc\n","  Using cached pyobjc-10.3.1-py3-none-any.whl.metadata (26 kB)\n","\u001b[31mERROR: Exception:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n","    return self.__dep_map\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n","    raise AttributeError(attr)\n","AttributeError: _DistInfoDistribution__dep_map\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n","    requirement_set = resolver.resolve(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n","    result = self._result = resolver.resolve(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n","    state = resolution.resolve(requirements, max_rounds=max_rounds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n","    self._add_to_criteria(self.state.criteria, r, parent=None)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n","    if not criterion.candidates:\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n","    return bool(self._sequence)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 174, in __bool__\n","    return any(self)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 162, in <genexpr>\n","    return (c for c in iterator if id(c) not in self._incompatible_ids)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 53, in _iter_built\n","    candidate = func()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 185, in _make_candidate_from_link\n","    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 231, in _make_base_candidate_from_link\n","    self._link_candidate_cache[link] = LinkCandidate(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 303, in __init__\n","    super().__init__(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n","    self.dist = self._prepare()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 247, in _prepare\n","    self._check_metadata_consistency(dist)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 229, in _check_metadata_consistency\n","    list(dist.iter_dependencies(list(dist.iter_provided_extras())))\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 250, in iter_provided_extras\n","    return self._extra_mapping.keys()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 94, in _extra_mapping\n","    canonicalize_name(extra): extra for extra in self._dist.extras\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3026, in extras\n","    return [dep for dep in self._dep_map if dep]\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n","    self.__dep_map = self._compute_dependencies()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3089, in _compute_dependencies\n","    common = types.MappingProxyType(dict.fromkeys(reqs_for_extra(None)))\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3086, in reqs_for_extra\n","    if not req.marker or req.marker.evaluate({'extra': extra}):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 325, in evaluate\n","    return _evaluate_markers(self._markers, current_environment)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 225, in _evaluate_markers\n","    groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 183, in _eval_op\n","    return spec.contains(lhs, prereleases=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 552, in contains\n","    normalized_item = _coerce_version(item)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 28, in _coerce_version\n","    version = Version(version)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/version.py\", line 202, in __init__\n","    raise InvalidVersion(f\"Invalid version: '{version}'\")\n","pip._vendor.packaging.version.InvalidVersion: Invalid version: '6.1.85+'\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-AMQrGxWa4EA"},"execution_count":null,"outputs":[]}]}